# [Hw05] Implementing a Multilayer Neural Networks using TensorFlow (TF) 1.x (Ch. 13)


## (1) Data preparation
- (1a) Load the train dataset and test dataset of the MNIST 
- (1b) Set the first 55,000 samples in the training dataset as training data, the remaining 5,000 samples in the training dataset as validation data, and 10,000 samples in the test dataset as test data.




## (2) Multilayer perceptron (MLP) using the low-level API of TF (e.g., p. 429)
### (2a) Implement the multilayer perceptron (MLP) with two hidden layers for classification, and evaluate performance in the following scenarios  
- (2a-1) for a few choices of hidden nodes per hidden layer
- (2a-2) for a few choices of activation functions (i.e., sigmoid, tanh, and ReLU)
    
### (2b) Add the L2-norm regularization of weights to (2a) and evalute performance for a few choices of L2-norm regularization parameters. For this, please pick the best-performing model from (2a)

### (2c) Evaluate the performance of (2b) for a few options of mini-batch sizes 
    
### (2d) Commonly for the results for (2a) - (2c), discuss the results such as by presenting convergence curves of cost and/or accuracy and by exemplifying the misclassified digits
    
